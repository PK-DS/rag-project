# -*- coding: utf-8 -*-
"""NITINBANKPROJGEM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15Q0bM0zG6cQimz1hwt5vSk5hHWKDspP7
"""

# =========================
# ONE-CELL FIXED GEMINI RAG
# =========================
!pip install -q google-generativeai faiss-cpu pypdf python-docx numpy

import os
os.environ["GEMINI_API_KEY"] = "AIzaSyDCsMK99uxNNN-5lZo0i3L7wmXY5dX45eo"

import google.generativeai as genai
genai.configure(api_key=os.environ["GEMINI_API_KEY"])

from pypdf import PdfReader
import docx
from pathlib import Path
import numpy as np
import faiss

# ---------- Load docs ----------
def load_documents(path="data"):
    docs = []
    for f in Path(path).glob("*"):
        if f.suffix == ".pdf":
            r = PdfReader(f)
            text = "\n".join(p.extract_text() for p in r.pages if p.extract_text())
        elif f.suffix == ".docx":
            d = docx.Document(f)
            text = "\n".join(p.text for p in d.paragraphs)
        elif f.suffix == ".txt":
            text = f.read_text()
        else:
            continue
        if text.strip():
            docs.append({"text": text, "source": f.name})
    return docs

# ---------- Chunk ----------
def chunk_text(text, size=500, overlap=100):
    out, i = [], 0
    while i < len(text):
        out.append(text[i:i+size])
        i += size - overlap
    return out

# ---------- Embed ----------
def embed_texts(texts):
    embs = []
    for t in texts:
        e = genai.embed_content(
            model="models/text-embedding-004",
            content=t
        )["embedding"]
        embs.append(e)
    return np.array(embs, dtype="float32")

# ---------- Build ----------
Path("data").mkdir(exist_ok=True)

docs = load_documents("/content")
chunks, meta = [], []

for d in docs:
    for c in chunk_text(d["text"]):
        chunks.append(c)
        meta.append({"document": d["source"], "chunk": c})

if len(chunks) == 0:
    raise RuntimeError("No documents/chunks found. Add files to ./data")

vectors = embed_texts(chunks)
dim = vectors.shape[1]

index = faiss.IndexFlatL2(dim)
index.add(vectors)

# ---------- Retrieve ----------
def retrieve(q, k=3):
    qv = embed_texts([q])
    _, idx = index.search(qv, k)
    return [meta[i] for i in idx[0]]

# ---------- Answer ----------
def answer(q):
    ctx = retrieve(q)
    context = "\n\n".join(c["chunk"] for c in ctx)
    model = genai.GenerativeModel("gemini-2.5-flash") # Changed model from gemini-1.5-flash to gemini-2.5-flash
    prompt = f"Answer ONLY from context. If not found say Not found.\n\nContext:\n{context}\n\nQ:{q}"
    return model.generate_content(prompt).text, ctx

# ---------- Run ----------
q = input("Ask: ")
ans, src = answer(q)
print("\nANSWER:\n", ans)
print("\nSOURCES:")
for s in src:
    print("-", s["document"])